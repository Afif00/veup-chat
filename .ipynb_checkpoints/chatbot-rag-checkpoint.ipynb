{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2a43e4-30d6-4d0a-b28c-177ab1bb9d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'flask_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from weaviate.classes.query import Filter\n",
    "import weaviate\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import flask_app\n",
    "import weaviate_server\n",
    "import unittests\n",
    "import json\n",
    "from utils import (\n",
    "    ChatWidget, \n",
    "    generate_with_single_input,\n",
    "    parse_json_output,\n",
    "    get_filter_by_metadata,\n",
    "    generate_filters_from_query,\n",
    "    process_and_print_query,\n",
    "    print_properties,\n",
    "    make_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c108e2-91e3-4a2e-8fa1-520be23e9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local(port=8079, grpc_port=50050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95d0ba0-03d4-4d59-ab7d-89190dc9d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:319: PydanticDeprecatedSince20: `json_encoders` is deprecated. See https://docs.pydantic.dev/2.12/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/alembic/config.py:598: DeprecationWarning: No path_separator found in configuration; falling back to legacy splitting on spaces, commas, and colons for prepend_sys_path.  Consider adding path_separator=os to Alembic config.\n",
      "  util.warn_deprecated(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
      "  next(self.gen)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFOLLOW THIS URL TO OPEN THE UI: http://127.0.0.1:8888\u001b[0m\n",
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "# Launch the lab and the URL\n",
    "make_url()\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088da294-2751-4a57-8eb4-2889a5b80442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: chatbot\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://127.0.0.1:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up the telemetry\n",
    "\n",
    "phoenix_project_name = \"chatbot\"\n",
    "\n",
    "# With phoenix, we just need to register to get the tracer provider with the appropriate endpoint. \n",
    "# Different from the ungraded lab, you will NOT use auto_instrument = True, as there are LLM calls not needed to be traced (examples, calls within unittests etc.)\n",
    "\n",
    "tracer_provider_phoenix = register(project_name=phoenix_project_name, endpoint=\"http://127.0.0.1:6006/v1/traces\")\n",
    "\n",
    "# Retrieve a tracer for manual instrumentation\n",
    "tracer = tracer_provider_phoenix.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8590d9-3126-4965-a905-717426162108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are your store hours?',\n",
       "  'answer': 'Our online store is open 24/7. Customer service is available from 9:00 AM to 6:00 PM, Monday through Friday.',\n",
       "  'type': 'general information'},\n",
       " {'question': 'Where is Fashion Forward Hub located?',\n",
       "  'answer': 'Fashion Forward Hub is primarily an online store. Our corporate office is located at 123 Fashion Lane, Trend City, Style State.',\n",
       "  'type': 'general information'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading products data\n",
    "products_data = joblib.load('dataset/clothes_json.joblib')\n",
    "# Let's get one example\n",
    "products_data[0]\n",
    "faq = joblib.load(\"dataset/faq.joblib\")\n",
    "# Get an example\n",
    "faq[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f59f89-010e-4d4b-9656-4dadf242e160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gender': 'Men',\n",
       "  'masterCategory': 'Apparel',\n",
       "  'subCategory': 'Topwear',\n",
       "  'articleType': 'Shirts',\n",
       "  'baseColour': 'Navy Blue',\n",
       "  'season': 'Fall',\n",
       "  'year': 2011,\n",
       "  'usage': 'Casual',\n",
       "  'productDisplayName': 'Turtle Check Men Navy Blue Shirt',\n",
       "  'price': 67.0,\n",
       "  'product_id': 15970},\n",
       " {'gender': 'Men',\n",
       "  'masterCategory': 'Apparel',\n",
       "  'subCategory': 'Bottomwear',\n",
       "  'articleType': 'Jeans',\n",
       "  'baseColour': 'Blue',\n",
       "  'season': 'Summer',\n",
       "  'year': 2012,\n",
       "  'usage': 'Casual',\n",
       "  'productDisplayName': 'Peter England Men Party Blue Jeans',\n",
       "  'price': 22.0,\n",
       "  'product_id': 39386}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27499ccf-fe68-43fd-a0ce-9f7458ff1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-CYp5li5jnoVY3N7WLOw45U5mvTLmL\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The primary colors can vary depending on the context in which they are used. Here are the two main systems:\\n\\n1. **Additive Color Model (used in light, such as screens)**: The primary colors are red, green, and blue (RGB). When combined, these colors can create a wide range of other colors, including white when all three are combined at full intensity.\\n\\n2. **Subtractive Color Model (used in pigments, such as paint)**: The primary colors are cyan, magenta, and yellow (CMY). When combined, these can create other colors, and when mixed together in equal parts ideally produce black (though in practice, a fourth color, black, is often added in printing, resulting in the CMYK model).\\n\\nIn traditional color theory related to art, the primary colors are often considered to be red, blue, and yellow.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1762415585,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_560af6e559\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 175,\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"total_tokens\": 188,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The output is a dictionary containing the role and content from the LLM call, as well as the token usage.:\n",
    "result = generate_with_single_input(\"What are the primary colors?\")\n",
    "print(json.dumps(result, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfd37c3-81bb-462c-a78d-9a79d7e4c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary colors can vary depending on the context in which they are used. Here are the two main systems:\n",
      "\n",
      "1. **Additive Color Model (used in light, such as screens)**: The primary colors are red, green, and blue (RGB). When combined, these colors can create a wide range of other colors, including white when all three are combined at full intensity.\n",
      "\n",
      "2. **Subtractive Color Model (used in pigments, such as paint)**: The primary colors are cyan, magenta, and yellow (CMY). When combined, these can create other colors, and when mixed together in equal parts ideally produce black (though in practice, a fourth color, black, is often added in printing, resulting in the CMYK model).\n",
      "\n",
      "In traditional color theory related to art, the primary colors are often considered to be red, blue, and yellow.\n"
     ]
    }
   ],
   "source": [
    "# To retreive the content, then you should do as follows:\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "164c76d6-b78d-42ed-993f-a59aa2acfd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "# The total tokens count (input + output) for this is:\n",
    "print(result['usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7720d06-fc1d-48c5-9f8f-03da5215373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    role: str = 'user',\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary of parameters for calling a Language Learning Model (LLM),\n",
    "    allowing for the customization of several key options that can affect the output from the model. \n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text that will be provided to the model to guide text generation.\n",
    "        temperature (float): A value between 0 and 1 that controls the randomness of the model's output; \n",
    "            lower values result in more repetitive and deterministic results, while higher values enhance randomness.\n",
    "        role (str): The role designation to be used in context, typically identifying the initiator of the interaction.\n",
    "        top_p (float): A value between 0 and 1 that manages diversity through the technique of nucleus sampling; \n",
    "            this parameter limits the set of considered words to the smallest possible while maintaining 'top_p' cumulative probability.\n",
    "        max_tokens (int): The maximum number of tokens that the model is allowed to generate in response, where a token can \n",
    "            be as short as one character or as long as one word.\n",
    "        model (str): The specific model identifier to be utilized for processing the request. This typically specifies both \n",
    "            the version and configuration of the LLM to be employed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all specified parameters which can then be used to configure and execute a call to the LLM.\n",
    "    \"\"\"\n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"role\": role,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"model\": model\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e92d388-1819-4db8-aab8-b00b797f1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    role: str = 'user',\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary of parameters for calling a Language Learning Model (LLM),\n",
    "    allowing for the customization of several key options that can affect the output from the model. \n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text that will be provided to the model to guide text generation.\n",
    "        temperature (float): A value between 0 and 1 that controls the randomness of the model's output; \n",
    "            lower values result in more repetitive and deterministic results, while higher values enhance randomness.\n",
    "        role (str): The role designation to be used in context, typically identifying the initiator of the interaction.\n",
    "        top_p (float): A value between 0 and 1 that manages diversity through the technique of nucleus sampling; \n",
    "            this parameter limits the set of considered words to the smallest possible while maintaining 'top_p' cumulative probability.\n",
    "        max_tokens (int): The maximum number of tokens that the model is allowed to generate in response, where a token can \n",
    "            be as short as one character or as long as one word.\n",
    "        model (str): The specific model identifier to be utilized for processing the request. This typically specifies both \n",
    "            the version and configuration of the LLM to be employed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all specified parameters which can then be used to configure and execute a call to the LLM.\n",
    "    \"\"\"\n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"role\": role,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"model\": model\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b021b1-e92c-494d-b56d-903c43d71428",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = generate_params_dict(\"Solve 3x^2 + 5 = 0\")\n",
    "print(kwargs)\n",
    "# Now you can call the LLM \n",
    "result = generate_with_single_input(**kwargs)\n",
    "content = result['choices'][0]['message']['content']\n",
    "total_tokens = result['usage']['total_tokens']\n",
    "print(f\"Content: {content}\\n\\nTotal Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0c56f-1db1-47f7-bea1-03b7dd303e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "def check_if_faq_or_product(query, simplified = False):\n",
    "    \"\"\"\n",
    "    Determines whether a given instruction prompt is related to a frequently asked question (FAQ) or a product inquiry.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The instruction or query that needs to be labeled as either FAQ or Product related.\n",
    "    - simplified (bool): If True, uses a simplified prompt.\n",
    "\n",
    "    Returns:\n",
    "    - str: The label 'FAQ' if the prompt is deemed a frequently asked question, 'Product' if it is related to product information, or\n",
    "      None if the label is inconclusive.\n",
    "    \"\"\"\n",
    " \n",
    "    # If not simplified, uses a more complex prompt\n",
    "    if not simplified:\n",
    "        PROMPT = f\"\"\"Label the following instruction as an FAQ related answer or a product related answer for a clothing store.\n",
    "        Product related answers are answers specific about product information or that needs to use the products to give an answer.\n",
    "        Examples:\n",
    "                Is there a refund for incorrectly bought clothes? Label: FAQ\n",
    "                Where are your stores located?: Label: FAQ\n",
    "                Tell me about the cheapest T-shirts that you have. Label: Product\n",
    "                Do you have blue T-shirts under 100 dollars? Label: Product\n",
    "                What are the available sizes for the t-shirts? Label: FAQ\n",
    "                How can I contact you via phone? Label: FAQ\n",
    "                How can I find the promotions? Label: FAQ\n",
    "                Give me ideas for a sunny look. Label: Product\n",
    "        Return only one of the two labels: FAQ or Product, nothing more.\n",
    "        Query to classify: {query}\n",
    "                 \"\"\"\n",
    "\n",
    "    ##############################################\n",
    "    ######### GRADED PART STARTS HERE ############\n",
    "    ##############################################\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # If simlpified, uses a simplified prompt.\n",
    "    else:\n",
    "        PROMPT = f\"\"\"\n",
    "                    Clothing store router: label the user message as **FAQ** or **Product**. Output exactly one word.\n",
    "\n",
    "FAQ = store policies/info and general size/availability questions.\n",
    "\n",
    "Product = requests to find/compare specific items in the catalog or styling advice using our items.\n",
    "\n",
    "Examples:\n",
    "Refund for incorrectly bought clothes â†’ FAQ\n",
    "Give me ideas for a sunny look â†’ Product\n",
    "\n",
    "User message: {query}\n",
    "\n",
    "                 \"\"\"\n",
    "\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    ##############################################\n",
    "    ######### GRADED PART ENDS HERE ############\n",
    "    ##############################################\n",
    "        \n",
    "    with tracer.start_as_current_span(\"routing_faq_or_product\", openinference_span_kind = 'tool') as span:\n",
    "        span.set_input(str({\"query\":query, \"simplified\": simplified}))\n",
    "        \n",
    "        # Get the kwargs dictinary to call the llm, with PROMPT as prompt, low temperature (0 or near 0) and max_tokens = 10\n",
    "        kwargs = generate_params_dict(PROMPT, temperature = 0, max_tokens = 10)\n",
    "\n",
    "        # Call generate_with_single_input with **kwargs\n",
    "        with tracer.start_as_current_span(\"router_call\", openinference_span_kind = 'llm') as router_span:\n",
    "            router_span.set_input(kwargs)\n",
    "            try:\n",
    "                response = generate_with_single_input(**kwargs) \n",
    "            except Exception as error:\n",
    "                router_span.record_exception(error)\n",
    "                router_span.set_status(Status(StatusCode.ERROR))\n",
    "            else:\n",
    "                # OpenInference Semantic Conventions for computing Costs\n",
    "                router_span.set_attribute(\"llm.token_count.prompt\", response['usage']['prompt_tokens'])\n",
    "                router_span.set_attribute(\"llm.token_count.completion\", response['usage']['completion_tokens'])\n",
    "                router_span.set_attribute(\"llm.token_count.total\", response['usage']['total_tokens'])\n",
    "                router_span.set_attribute(\"llm.model_name\", response['model'])\n",
    "                router_span.set_attribute(\"llm.provider\", 'together.ai')\n",
    "                router_span.set_output(response)\n",
    "                router_span.set_status(Status(StatusCode.OK))\n",
    "        \n",
    "    \n",
    "        # Get the Label by accessing the content key of the response dictionary\n",
    "        label = response['choices'][0]['message']['content']\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        span.set_output(str({\"label\": label, 'total_tokens':total_tokens}))\n",
    "        span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "        # Improvement to prevent cases where LLM outputs more than one word\n",
    "        if 'faq' in label.lower():\n",
    "            label = 'FAQ'\n",
    "        elif 'product' in label.lower():\n",
    "            label = 'Product'\n",
    "        else:\n",
    "            label = 'undefined'\n",
    "    \n",
    "        return label, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eada425-d8ea-4302-bc94-b48c96387ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unittests.test_check_if_faq_or_product(check_if_faq_or_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d204e0-daa9-4a91-a07e-0e727d23c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'What is your return policy?', \n",
    "    'Give me three examples of blue T-shirts you have available.', \n",
    "    'How can I contact the user support?', \n",
    "    'Do you have blue Dresses?',\n",
    "    'Create a look suitable for a wedding party happening during dawn.'\n",
    "]\n",
    "\n",
    "labels = ['FAQ', 'Product', 'FAQ', 'Product', 'Product']\n",
    "\n",
    "for query, correct_label in zip(queries, labels):\n",
    "    # Call check_if_faq_or_product and store the results\n",
    "    response_std, tokens_std = check_if_faq_or_product(query, simplified=False)\n",
    "    response_simp, tokens_simp = check_if_faq_or_product(query, simplified=True)\n",
    "    \n",
    "    # Print results\n",
    "    process_and_print_query(query, correct_label, response_std, tokens_std, response_simp, tokens_simp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f481654-6c84-451e-a746-66114e4e60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool\n",
    "def generate_faq_layout(faq_dict):\n",
    "    \"\"\"\n",
    "    Generates a formatted string layout for a list of FAQs.\n",
    "\n",
    "    This function iterates through a dictionary of frequently asked questions (FAQs) and constructs\n",
    "    a string where each question is followed by its corresponding answer and type.\n",
    "\n",
    "    Parameters:\n",
    "    - faq_dict (list): A list of dictionaries, each containing keys 'question', 'answer', and 'type' \n",
    "      representing an FAQ entry.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string representing the formatted layout of FAQs, with each entry on a separate line.\n",
    "    \"\"\"\n",
    "    # Initialize an empty string\n",
    "    t = \"\"\n",
    "    \n",
    "    # Iterate over every FAQ question in the FAQ list\n",
    "    for f in faq_dict:\n",
    "        # Append the question with formatted string (remember to use f-string and access the values as f['question'], f['answer'] and so on)\n",
    "        # Also, do not forget to add a new line character (\\n) at the end of each line.\n",
    "        t += f\"Question: {f['question']} Answer: {f['answer']} Type: {f['type']}\\n\" \n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcb833-d6a4-4a95-8f80-4786759794d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can generate a full faq_layout with the entire FAQ questions\n",
    "faq_layout = generate_faq_layout(faq)\n",
    "print(faq_layout[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76117cc-d55a-4bcc-8745-afdb51a842f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose some faq questions and generate a layout from them. \n",
    "# They just need to be in a list with dictionaries with the necessary keys: 'question', 'answer' and 'type'\n",
    "print(generate_faq_layout(faq[1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc5a6-985d-444f-9dad-a516e080d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_collection = client.collections.get(\"Faq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a8e7e-91db-49f6-a1e4-017879fbec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from weaviate.util import generate_uuid5\n",
    "# Set up a batch process with specified fixed size and concurrency\n",
    "with faq_collection.batch.fixed_size(batch_size=20, concurrent_requests=5) as batch:\n",
    "    # Iterate over a subset of the dataset\n",
    "    for document in tqdm(faq):\n",
    "        # Generate a UUID based on the chunk text for unique identification\n",
    "        uuid = generate_uuid5(document['question'])\n",
    "\n",
    "        # Add the chunk object to the batch with properties and UUID\n",
    "        batch.add_object(\n",
    "            properties=document,\n",
    "            uuid=uuid,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa6034-46c2-4ed3-9421-6f4913d6b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = faq_collection.query.near_text(\"What is the return policy?\", limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2dae0-f9d1-485d-81b9-6f66defc22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in res.objects:\n",
    "    print_properties(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f72f08-e665-4b0b-823a-e302a6c20c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def query_on_faq(query, simplified = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a prompt to query an FAQ system and generates a response.\n",
    "\n",
    "    This function integrates an FAQ layout into the prompt to help generate a suitable answer to the given query\n",
    "    using a language model. It supports additional keyword arguments to customize the prompt generation process.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The query about which the function seeks to provide an answer from the FAQ.\n",
    "    - simplified (bool): If True, uses semantic search to extract a relevant subset of FAQ questions\n",
    "    - **kwargs: Optional keyword arguments for extra configuration of prompt parameters.\n",
    "\n",
    "    Returns:\n",
    "    - str: The response generated from the language model based on the input query and FAQ layout.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # If not simplified, generate the faq layout with the entire FAQ questions\n",
    "    if not simplified:\n",
    "        # Set the tracer as a chain type, since in non-simplified version, the full FAQ is used\n",
    "        with tracer.start_as_current_span(\"query_on_faq\", openinference_span_kind=\"tool\") as span:\n",
    "            \n",
    "            span.set_input({\"query\": query, \"simplified\": simplified})\n",
    "            faq_layout = generate_faq_layout(faq)\n",
    "            \n",
    "            # Generate the prompt\n",
    "            PROMPT = f\"\"\"You will be provided with an FAQ for a clothing store. \n",
    "        Answer the instruction based on it. You might use more than one question and answer to make your answer. Only answer the question and do not mention that you have access to a FAQ. \n",
    "        <FAQ_ITEMS>\n",
    "        PROVIDED FAQ: {faq_layout}\n",
    "        </FAQ_ITEMS>\n",
    "        Question: {query}\n",
    "            \"\"\" \n",
    "            span.set_attribute(\"prompt\", PROMPT)\n",
    "\n",
    "            # Generate the parameters dict with PROMPT and **kwargs \n",
    "            kwargs = generate_params_dict(PROMPT, **kwargs) \n",
    "\n",
    "            span.set_attribute(\"output\", str(kwargs))\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "    \n",
    "            return kwargs\n",
    "        \n",
    "   \n",
    "    \n",
    "    else:\n",
    "        with tracer.start_as_current_span(\"query_on_faq\", openinference_span_kind=\"tool\") as span:\n",
    "            span.set_input({\"query\": query, \"simplified\": simplified})\n",
    "            with tracer.start_as_current_span(\"retrieve_faq_questions\", openinference_span_kind=\"retriever\") as retrieve_span:\n",
    "                \n",
    "                ##############################################\n",
    "                ######### GRADED PART STARTS HERE ############\n",
    "                ##############################################\n",
    "                \n",
    "                ### START CODE HERE ###\n",
    "                \n",
    "                # Get the 5 most relevant FAQ objects, in this case limit = 5\n",
    "                results = faq_collection.query.near_text(query, limit=5)\n",
    "\n",
    "                ### END CODE HERE ###\n",
    "\n",
    "                ##############################################\n",
    "                ######### GRADED PART ENDS HERE ##############\n",
    "                ##############################################\n",
    "                \n",
    "                # Set the retrieved documents as attributes on the span\n",
    "                for i, document in enumerate(results.objects): \n",
    "                    retrieve_span.set_attribute(f\"retrieval.documents.{i}.document.id\", str(document.uuid)) \n",
    "                    retrieve_span.set_attribute(f\"retrieval.documents.{i}.document.metadata\", str(document.metadata)) \n",
    "                    retrieve_span.set_attribute( \n",
    "                        f\"retrieval.documents.{i}.document.content\", str(document.properties) \n",
    "                    )  \n",
    "            # Transform the results in a list of dictionary\n",
    "                results = [x.properties for x in results.objects] \n",
    "                # Reverse the order to add the most relevant objects in the bottom, so it gets closer to the end of the input\n",
    "                results.reverse() \n",
    "                # Generate the faq layout with the new list of FAQ questions `results`\n",
    "                faq_layout = generate_faq_layout(results) \n",
    "\n",
    "            # Different prompt to deal with this new scenario. \n",
    "            PROMPT = (f\"You will be provided with a query for a clothing store regarding FAQ. It will be provided relevant FAQ from the clothing store.\" \n",
    "        f\"Answer the query based on the relevant FAQ provided. They are ordered in decreasing relevance, so the first is the most relevant FAQ and the last is the least relevant.\"  \n",
    "        f\"Answer the instruction based on them. You might use more than one question and answer to make your answer. Only answer the question and do not mention that you have access to a FAQ.\\n\"  \n",
    "        f\"<FAQ>\\n\"  \n",
    "        f\"RELEVANT FAQ ITEMS:\\n{faq_layout}\\n\"  \n",
    "        f\"</FAQ>\\n\" \n",
    "        f\"Query: {query}\")\n",
    "\n",
    "    \n",
    "        \n",
    "            span.set_attribute(\"prompt\", PROMPT)\n",
    "        \n",
    "            # Generate the parameters dict with PROMPT and **kwargs \n",
    "            kwargs = generate_params_dict(PROMPT, **kwargs) \n",
    "        \n",
    "            span.set_attribute(\"output\", str(kwargs))\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "    \n",
    "            return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1caae2-af5f-4218-89e2-c50630bb9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unittests.test_query_on_faq(query_on_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11b550-1591-4e81-abdc-b1f42cb1121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dictionary of arguments\n",
    "kwargs = query_on_faq(\"I received the dress I ordered but I don't like it. How can I return it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84817c53-e55d-40c0-a86e-dade0b37449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of split tokens in this prompt is:\n",
    "print(len(kwargs['prompt'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daecb5f-0ff8-469b-9ac4-98914140d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference\n",
    "result = generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5329b-5516-4f00-a392-0c3a4bf1271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c226a0c-9c5a-4445-a299-694bc6ead96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total tokens\n",
    "print(result['usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb320c7e-e082-4de0-a45d-9e188d7dea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dictionary of arguments\n",
    "kwargs = query_on_faq(\"I received the dress I ordered but I don't like it. How can I return it?\", simplified = True)\n",
    "# The number of split tokens in this prompt is:\n",
    "print(len(kwargs['prompt'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab0e6c-9cf1-4524-9755-c3bdb8cd4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d5627-c0f9-4416-a61e-600782cf14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total tokens\n",
    "print(result['usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a4678-a41b-4e31-ad7b-25f27adad4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "def decide_task_nature(query, simplified = True):\n",
    "    \"\"\"\n",
    "    Determines the nature of a query, labeling it as either creative or technical.\n",
    "\n",
    "    This function constructs a prompt for a language model to decide if a given query requires a creative response,\n",
    "    such as making suggestions or composing ideas, or a technical response, like providing product details or prices.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The query to be evaluated for its nature.\n",
    "    - simplified (bool): If True, uses a simplified prompt.\n",
    "\n",
    "    Returns:\n",
    "    - str: The label 'creative' if the query requires creative input, or 'technical' if it requires technical information.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    if not simplified:\n",
    "        PROMPT = f\"\"\"Decide if the following query is a query that requires creativity (creating, composing, making new things) or technical (information about products, prices etc.). Label it as creative or technical.\n",
    "          Examples:\n",
    "          Give me suggestions on a nice look for a nightclub. Label: creative\n",
    "          What are the blue dresses you have available? Label: technical\n",
    "          Give me three Tshirts for summer. Label: technical\n",
    "          Give me a look for attending a wedding party. Label: creative\n",
    "          Give me suggestions on clothes that match a green Tshirt. Label: creative\n",
    "          I would like a suggestion on which products match a green Tshirt I already have. Label: creative\n",
    "\n",
    "          Query to be analyzed: {query}. Only output one token with the label\n",
    "          \"\"\"\n",
    "\n",
    "    # If simplified, uses a simplified query\n",
    "\n",
    "    ##############################################\n",
    "    ######### GRADED PART STARTS HERE ############\n",
    "    ##############################################\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    else:\n",
    "        PROMPT = f\"\"\"Decide if the following query is a query that requires creativity or technical . Label it as creative or technical.\n",
    "          Examples:\n",
    "          Give me suggestions on a nice look for a nightclub. Label: creative\n",
    "          What are the blue dresses you have available? Label: technical\n",
    "          Give me three Tshirts for summer. Label: technical\n",
    "\n",
    "          Query to be analyzed: {query}. Only output one token with the label\n",
    "          \"\"\"\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    ##############################################\n",
    "    ######### GRADED PART ENDS HERE ##############\n",
    "    ##############################################\n",
    "\n",
    "    \n",
    "    with tracer.start_as_current_span(\"decide_task_nature\", openinference_span_kind=\"tool\") as span:\n",
    "    # Generate the kwards dictionary by passing the PROMPT, low temperature and max_tokens = 1\n",
    "        span.set_input({\"query\":query, \"simplified\": simplified})\n",
    "        kwargs = generate_params_dict(PROMPT, temperature = 0, max_tokens = 1)\n",
    "\n",
    "        with tracer.start_as_current_span(\"router_call\", openinference_span_kind = 'llm') as router_span:\n",
    "            router_span.set_input(kwargs)\n",
    "            try:\n",
    "                response = generate_with_single_input(**kwargs) \n",
    "            except Exception as error:\n",
    "                router_span.record_exception(error)\n",
    "                router_span.set_status(Status(StatusCode.ERROR))\n",
    "            else:\n",
    "                # OpenInference Semantic Conventions for computing Costs\n",
    "                router_span.set_attribute(\"llm.token_count.prompt\", response['usage']['prompt_tokens'])\n",
    "                router_span.set_attribute(\"llm.token_count.completion\", response['usage']['completion_tokens'])\n",
    "                router_span.set_attribute(\"llm.token_count.total\", response['usage']['total_tokens'])\n",
    "                router_span.set_attribute(\"llm.model_name\", response['model'])\n",
    "                router_span.set_attribute(\"llm.provider\", 'together.ai')\n",
    "                router_span.set_output(response)\n",
    "                router_span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "        # Get the Label by accessing the content key of the response dictionary\n",
    "        label = response['choices'][0]['message']['content']\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        span.set_output(str({\"label\": label, 'total_tokens':total_tokens}))\n",
    "        span.set_status(Status(StatusCode.OK))    \n",
    "    \n",
    "        return label, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5e35a-2838-49ce-9638-47cf8ee3e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unittests.test_decide_task_nature(decide_task_nature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e988c-7155-4176-acc6-bec0d171fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Give me two sneakers with vibrant colors.\",\n",
    "           \"What are the most expensive clothes you have in your catalogue?\",\n",
    "           \"I have a green Dress and I like a suggestion on an accessory to match with it.\",\n",
    "           \"Give me three trousers with vibrant colors you have in your catalogue.\",\n",
    "           \"Create a look for a woman walking in a park on a sunny day. It must be fresh due to hot weather.\"\n",
    "           ]\n",
    "\n",
    "labels = ['technical', 'technical', 'creative', 'technical', 'creative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ab32d-be9d-496c-ab73-8a48a2394369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, correct_label in zip(queries, labels):\n",
    "    response, total_tokens = decide_task_nature(query, simplified = True)\n",
    "    label = response\n",
    "    if label == correct_label:\n",
    "        label = \"\\033[32m\" + label + \"\\033[0m\" \n",
    "    else:\n",
    "        label = \"\\033[31m\" + label + \"\\033[0m\"\n",
    "    if total_tokens > 170:\n",
    "        total_tokens = \"\\033[31m\"  + str(total_tokens) + \"\\033[0m\"\n",
    "    else:\n",
    "        total_tokens = \"\\033[32m\"  + str(total_tokens) + \"\\033[0m\"\n",
    "    print(f\"Query: {query} Label Predicted: {label}. Correct Label: {correct_label} Total Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ed49a-38ce-4a53-9306-66dad6cdcb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool\n",
    "def get_params_for_task(task):\n",
    "    \"\"\"\n",
    "    Retrieves specific language model parameters based on the task nature.\n",
    "\n",
    "    This function provides parameter sets tailored for creative or technical tasks to optimize\n",
    "    language model behavior. For creative tasks, higher randomness is encouraged, while technical\n",
    "    tasks are handled with more focus and precision. A default parameter set is provided for unexpected cases.\n",
    "\n",
    "    Parameters:\n",
    "    - task (str): The nature of the task ('creative' or 'technical').\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing 'top_p' and 'temperature' settings for the specified task.\n",
    "    \"\"\"\n",
    "    # Create the parameters dict for technical and creative tasks\n",
    "    PARAMETERS_DICT = {\"creative\": {'top_p': 0.9, 'temperature': 1},\n",
    "                       \"technical\": {'top_p': 0.7, 'temperature': 0.3}} \n",
    "    \n",
    "    # If task is technical, return the value for the key technical in PARAMETERS_DICT\n",
    "    if task == 'technical':\n",
    "        param_dict = PARAMETERS_DICT['technical'] \n",
    "\n",
    "    # If task is creative, return the value for the key creative in PARAMETERS_DICT\n",
    "    if task == 'creative':\n",
    "        param_dict = PARAMETERS_DICT['creative'] \n",
    "\n",
    "    # If task is a different value, fallback to another set of parameters\n",
    "    else: # Fallback to a standard value\n",
    "        param_dict = {'top_p': 0.5, 'temperature': 1} \n",
    "\n",
    "    \n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e9105d-1892-4bdb-b716-c2441727d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'Men',\n",
       " 'masterCategory': 'Apparel',\n",
       " 'subCategory': 'Topwear',\n",
       " 'articleType': 'Shirts',\n",
       " 'baseColour': 'Navy Blue',\n",
       " 'season': 'Fall',\n",
       " 'year': 2011,\n",
       " 'usage': 'Casual',\n",
       " 'productDisplayName': 'Turtle Check Men Navy Blue Shirt',\n",
       " 'price': 67.0,\n",
       " 'product_id': 15970}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remember the data structure of a product\n",
    "products_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd194edd-e367-400e-af42-30c2b36f4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the dictionary with the possible values for each key\n",
    "values = {}\n",
    "for d in products_data:\n",
    "    for key, val in d.items():\n",
    "        if key in ('product_id', 'price', 'productDisplayName', 'subCategory', 'year'):\n",
    "            continue\n",
    "        if key not in values.keys():\n",
    "            values[key] = set()\n",
    "        values[key].add(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca812e2f-9642-4542-a053-1caac32a17a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All seasons', 'Fall', 'Spring', 'Summer', 'Winter'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of possible values for the feature 'season'\n",
    "values['season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c0c840-f676-4436-9170-03d410068950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata_from_query(query):\n",
    "    \"\"\"\n",
    "    Generates metadata in JSON format based on a given query to filter clothing items.\n",
    "\n",
    "    This function constructs a prompt for a language model to create a JSON object that will\n",
    "    guide the filtering of a vector database query for clothing items. It takes possible values from\n",
    "    a predefined set and ensures only relevant metadata is included in the output JSON.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The query describing specific clothing-related needs.\n",
    "\n",
    "    Returns:\n",
    "    - str: A JSON string representing metadata with keys like gender, masterCategory, articleType,\n",
    "      baseColour, price, usage, and season. Each value in the JSON is within a list, with prices specified\n",
    "      as a dict containing \"min\" and \"max\" values. Unrestricted keys should use [\"Any\"] and unspecified\n",
    "      prices should default to {\"min\": 0, \"max\": \"inf\"}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the prompt. Remember to include the query, the desired JSON format, the possible values (passing {values} at some point) \n",
    "    # and explain to the LLM what is going on. \n",
    "    # Explicitly tell the llm to include gender, masterCategory, ArticleType, baseColour, price, usage and season as keys.\n",
    "    # Also mention to the llm that price key must be a json with \"min\" and \"max\" values (0 if no lower bound and inf if no upper bound)\n",
    "    # If there is no price set, add min = 0 and max = inf.\n",
    "    PROMPT = f\"\"\"\n",
    "    One query will be provided. For the given query, there will be a call on vector database to query relevant clothing items. \n",
    "    Generate a JSON with useful metadata to filter the products in the query. Possible values for each feature is in the following json: {values}\n",
    "\n",
    "    Provide a JSON with the features that best fit in the query (can be more than one, write in a list). Also, if present, add a price key, saying if there is a price range (between values, greater than or smaller than some value).\n",
    "    Only return the JSON, nothing more. price key must be a JSON with \"min\" and \"max\" values (0 if no lower bound and inf if no upper bound). \n",
    "    Always include gender, masterCategory, articleType, baseColour, price, usage and season as keys. All values must be within lists.\n",
    "    If there is no price set, add min = 0 and max = inf.\n",
    "    Only include values that are given in the json above. \n",
    "    \n",
    "    Example of expected JSON:\n",
    "\n",
    "    {{\n",
    "    \"gender\": [\"Women\"],\n",
    "    \"masterCategory\": [\"Apparel\"],\n",
    "    \"articleType\": [\"Dresses\"],\n",
    "    \"baseColour\": [\"Blue\"],\n",
    "    \"price\": {{\"min\": 0, \"max\": \"inf\"}},\n",
    "    \"usage\": [\"Formal\"],\n",
    "    \"season\": [\"All seasons\"]\n",
    "    }}\n",
    "\n",
    "    Query: {query}\n",
    "             \"\"\"\n",
    "    with tracer.start_as_current_span(\"generate_metadata_from_query\", openinference_span_kind=\"tool\") as span:\n",
    "        span.set_input(query)\n",
    "        with tracer.start_as_current_span(\"llm_call\", openinference_span_kind=\"llm\") as metadata_span:\n",
    "            # Generate the response with the generate_with_single_input, PROMPT, temperature = 0 (low randomness) and max_tokens = 1500.\n",
    "            kwargs = {\"prompt\": PROMPT, 'temperature': 0, \"max_tokens\": 1500}  # @REPLACE EQUALS None\n",
    "            metadata_span.set_input(kwargs)\n",
    "            try:\n",
    "                response = generate_with_single_input(**kwargs) \n",
    "            except Exception as error:\n",
    "                metadata_span.record_exception(error)\n",
    "                metadata_span.set_status(Status(StatusCode.ERROR))\n",
    "            else:\n",
    "                # OpenInference Semantic Conventions for computing Costs\n",
    "                metadata_span.set_attribute(\"llm.token_count.prompt\", response['usage']['prompt_tokens'])\n",
    "                metadata_span.set_attribute(\"llm.token_count.completion\", response['usage']['completion_tokens'])\n",
    "                metadata_span.set_attribute(\"llm.token_count.total\", response['usage']['total_tokens'])\n",
    "                metadata_span.set_attribute(\"llm.model_name\", response['model'])\n",
    "                metadata_span.set_attribute(\"llm.provider\", 'together.ai')\n",
    "                metadata_span.set_output(response)\n",
    "                metadata_span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "        # Get the Label by accessing the content key of the response dictionary\n",
    "        content = response['choices'][0]['message']['content']\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        span.set_output({\"content\": content, 'total_tokens':total_tokens})\n",
    "        span.set_status(Status(StatusCode.OK))   \n",
    "\n",
    "    \n",
    "    return content, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b827519e-f9a8-4b45-86b0-2f4275b25f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "content, total_tokens = generate_metadata_from_query(\"Create a look for a man that suits a sunny day in the park. I don't want to spend more than 300 dollars on each piece.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9e952ca-cdf7-4d91-a213-0531f6075b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gender\": [\"Men\"],\n",
      "    \"masterCategory\": [\"Apparel\"],\n",
      "    \"articleType\": [\"Tshirts\", \"Shorts\", \"Casual Shoes\", \"Hats\"],\n",
      "    \"baseColour\": [\"White\", \"Blue\", \"Grey\", \"Green\", \"Yellow\"],\n",
      "    \"price\": {\"min\": 0, \"max\": 300},\n",
      "    \"usage\": [\"Casual\"],\n",
      "    \"season\": [\"Summer\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eca76e3-8731-4e5e-b4d1-8e260d9ee177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398\n"
     ]
    }
   ],
   "source": [
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "971a2548-e44c-45ad-82e0-540a7eed87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_collection = client.collections.get('Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd32e1a-6f70-4e42-a1ad-cbb735776952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce8c287a-505e-41ba-bd97-aed8d66803da",
   "metadata": {},
   "outputs": [
    {
     "ename": "WeaviateQueryError",
     "evalue": "Query call with protocol GRPC search failed with message explorer: get class: concurrentTargetVectorSearch): explorer: get class: vectorize search vector: vectorize params: vectorize params: vectorize keywords: remote client vectorize: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/connect/v4.py:985\u001b[39m, in \u001b[36mConnectionSync.grpc_search\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grpc_stub \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m res = \u001b[43m_Retry\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_exponential_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSearching in collection \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrpc_stub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrpc_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(search_get_pb2.SearchReply, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/retry.py:54\u001b[39m, in \u001b[36m_Retry.with_exponential_backoff\u001b[39m\u001b[34m(self, count, error, f, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err.code() != StatusCode.UNAVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     55\u001b[39m logger.info(\n\u001b[32m     56\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m received exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Retrying with exponential backoff in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m2\u001b[39m**count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/retry.py:50\u001b[39m, in \u001b[36m_Retry.with_exponential_backoff\u001b[39m\u001b[34m(self, count, error, f, *args, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/grpc/_channel.py:1181\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1175\u001b[39m (\n\u001b[32m   1176\u001b[39m     state,\n\u001b[32m   1177\u001b[39m     call,\n\u001b[32m   1178\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1179\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1180\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/grpc/_channel.py:1006\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"explorer: get class: concurrentTargetVectorSearch): explorer: get class: vectorize search vector: vectorize params: vectorize params: vectorize keywords: remote client vectorize: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"explorer: get class: concurrentTargetVectorSearch): explorer: get class: vectorize search vector: vectorize params: vectorize params: vectorize keywords: remote client vectorize: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY\", grpc_status:2}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mWeaviateQueryError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mproducts_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnear_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblue t shirt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/collections/queries/near_text/query/executor.py:424\u001b[39m, in \u001b[36m_NearTextQueryExecutor.near_text\u001b[39m\u001b[34m(self, query, certainty, distance, move_to, move_away, limit, offset, auto_limit, filters, group_by, rerank, target_vector, include_vector, return_metadata, return_properties, return_references)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         Any,\n\u001b[32m    393\u001b[39m         \u001b[38;5;28mself\u001b[39m._result_to_query_or_groupby_return(\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m         ),\n\u001b[32m    405\u001b[39m     )\n\u001b[32m    407\u001b[39m request = \u001b[38;5;28mself\u001b[39m._query.near_text(\n\u001b[32m    408\u001b[39m     near_text=query,\n\u001b[32m    409\u001b[39m     certainty=certainty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     return_references=\u001b[38;5;28mself\u001b[39m._parse_return_references(return_references),\n\u001b[32m    423\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrpc_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/connect/executor.py:99\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(method, response_callback, exception_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp_call\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(T, \u001b[43mexception_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/connect/executor.py:38\u001b[39m, in \u001b[36mraise_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_exception\u001b[39m(e: \u001b[38;5;167;01mException\u001b[39;00m) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/connect/executor.py:80\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(method, response_callback, exception_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\n\u001b[32m     72\u001b[39m     method: SyncOrAsyncMethod[P, R],\n\u001b[32m     73\u001b[39m     response_callback: SyncOrAsyncCallback[R, T, A],\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m ) -> Union[T, Awaitable[T], Awaitable[A]]:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# wrap method call in try-except to catch exceptions for sync method\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         call = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(call, Awaitable):\n\u001b[32m     83\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute\u001b[39m() -> T:\n\u001b[32m     84\u001b[39m                 \u001b[38;5;66;03m# wrap await in try-except to catch exceptions for async method\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.11/site-packages/weaviate/connect/v4.py:998\u001b[39m, in \u001b[36mConnectionSync.grpc_search\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m error.code() == StatusCode.PERMISSION_DENIED:\n\u001b[32m    997\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InsufficientPermissionsError(error)\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateQueryError(\u001b[38;5;28mstr\u001b[39m(error.details()), \u001b[33m\"\u001b[39m\u001b[33mGRPC search\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m WeaviateRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateQueryError(\u001b[38;5;28mstr\u001b[39m(e), \u001b[33m\"\u001b[39m\u001b[33mGRPC search\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mWeaviateQueryError\u001b[39m: Query call with protocol GRPC search failed with message explorer: get class: concurrentTargetVectorSearch): explorer: get class: vectorize search vector: vectorize params: vectorize params: vectorize keywords: remote client vectorize: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY."
     ]
    }
   ],
   "source": [
    "res = products_collection.query.near_text(\"blue t shirt\", limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c817c8-9b19-402f-b425-675359792e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
